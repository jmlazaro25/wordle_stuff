{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184a04ae-a62d-45ec-b226-cbbf5eb975f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e1592f-1e2b-4bf0-aa2d-fa4ff6e2db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Checkpointer():\n",
    "    def __init__(self, dir, model_file, optim_file):\n",
    "        self.dir = dir\n",
    "        self.model_file = os.path.join(self.dir, model_file)\n",
    "        self.optim_file = os.path.join(self.dir, optim_file)\n",
    "        self.best_loss = None\n",
    "\n",
    "    def checkpoint(self, loss, model, optim) -> None:\n",
    "        if self.best_loss is None or loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            model.eval()\n",
    "            torch.save(model.state_dict(), self.model_file)\n",
    "            torch.save(optim.state_dict(), self.optim_file)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, min_delta=0, patience=0, start_episode=0):\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.start_episode = start_episode\n",
    "        self.episode_last_improved = 0\n",
    "        self.best_loss = None\n",
    "        self.stopped = False\n",
    "\n",
    "    def stop(self, loss, episode_n) -> bool:\n",
    "        if self.best_loss is None or loss <= self.loss - self.min_delta:\n",
    "            self.loss = loss\n",
    "            self.episode_last_improved = episode_n\n",
    "        elif (\n",
    "            episode_n > start_episode + self.patience\n",
    "            and episode_n > sef.episode_last_improved + self.patience\n",
    "        ):\n",
    "            self.stopped = True\n",
    "        return self.stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e53196-0e47-49ba-89b3-2d50f1e19079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = '/'.join(os.getcwd().split('/')[:-1])\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from wordle import Wordle\n",
    "from wordle import load_vocab\n",
    "from wordle import ALPH_LEN\n",
    "\n",
    "from itertools import pairwise\n",
    "from numpy import prod\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Iterable\n",
    "from typing import Union\n",
    "from numpy import ndarray\n",
    "from numpy import int64\n",
    "\n",
    "training_history = dict[str, tuple[float]]\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'is_terminal'))\n",
    "\n",
    "ConvPoolConfig = namedtuple(\n",
    "    'ConvPoolConfig',\n",
    "    (\n",
    "        'conv_out',\n",
    "        'conv_kernel_size',\n",
    "        'pool_kernel_size',\n",
    "        'pool_padding',\n",
    "        'dropout'\n",
    "    )\n",
    ")\n",
    "\n",
    "FCConfig = namedtuple('FCConfig', ('f_out', 'dropout'))\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    n_char_channels = 1 + ALPH_LEN\n",
    "    n_hint_channels = Wordle.n_hint_states\n",
    "    channels_in = n_char_channels + n_hint_channels\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_len: int,\n",
    "        n_actions: int,\n",
    "        conv_pool_configs: Iterable[ConvPoolConfig],\n",
    "        fc_configs: list[FCConfig]\n",
    "    ) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        self.target_len = target_len\n",
    "        self.input_size = DQN.get_input_size(self.target_len)\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        if self.n_actions != fc_configs[-1].f_out:\n",
    "            raise ValueError(f'fc_configs[-1].f_out ({fc_configs[-1].f_out}) should equal n_actions ({n_actions})')\n",
    "\n",
    "        self.conv_pool_configs = conv_pool_configs\n",
    "        self.fc_configs = fc_configs\n",
    "\n",
    "        self.conv_pool_layers = DQN.make_conv_pool_layers(self.conv_pool_configs)\n",
    "        self.fcin = DQN.get_fcin(self.input_size, self.conv_pool_layers)\n",
    "        self.fc_layers = DQN.make_fc_layers(self.n_actions, self.fc_configs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.one_hot_state(x)\n",
    "        x = torch.cat(x)\n",
    "        x = self.conv_pool_layers(x)\n",
    "        x = x.view(-1, self.fcin)  # -1 for batch size inference\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def make_conv_pool_layers(conv_pool_configs: Iterable[ConvPoolConfig]) -> nn.Sequential:\n",
    "        first_config = conv_pool_configs[0]\n",
    "        conv_pool_layers = [\n",
    "                nn.Conv2d(DQN.channels_in, first_config.conv_out, kernel_size=first_config.conv_kernel_size, padding='same'),\n",
    "                nn.ReLU()\n",
    "        ]\n",
    "        if first_config.pool_kernel_size:\n",
    "            conv_pool_layers.append(\n",
    "                nn.MaxPool2d(kernel_size=first_config.pool_kernel_size, stride=1, padding=first_config.pool_padding)\n",
    "            )\n",
    "        if first_config.dropout:\n",
    "            conv_pool_layers.append(nn.Dropout2d(first_config.dropout))\n",
    "\n",
    "        for config1, config2 in pairwise(conv_pool_configs):\n",
    "            conv_pool_layers.extend((\n",
    "                nn.Conv2d(config1.conv_out, config2.conv_out, kernel_size=config2.conv_kernel_size, padding='same'),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "            if config2.pool_kernel_size:\n",
    "                conv_pool_layers.append(\n",
    "                    nn.MaxPool2d(kernel_size=config2.pool_kernel_size, stride=1, padding=config2.pool_padding)\n",
    "                )\n",
    "            if config2.dropout:\n",
    "                conv_pool_layers.append(nn.Dropout2d(config2.dropout))\n",
    "        return nn.Sequential(*conv_pool_layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_size(target_len):\n",
    "        return (1, DQN.channels_in, Wordle.max_attempts, target_len)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fcin(input_size: tuple[int], conv_pool_layers: nn.Sequential) -> int:\n",
    "        test_tensor = torch.zeros(input_size)\n",
    "        with torch.no_grad():\n",
    "            test_out_size = conv_pool_layers(test_tensor).size()\n",
    "        return prod(test_out_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_fc_layers(n_actions, fc_configs: list[FCConfig]) -> nn.Sequential:\n",
    "        first_config = fc_configs[0]\n",
    "\n",
    "        fc_layers = [nn.Linear(fcin, first_config.f_out)]\n",
    "        if len(fc_configs) > 1:\n",
    "            fc_layers.append(nn.ReLU())\n",
    "        if first_config.dropout:\n",
    "            fc_layers.append(nn.Dropout(first_config.dropout))\n",
    "\n",
    "        for config1, config2 in pairwise(fc_configs):\n",
    "            fc_layers.append(nn.Linear(config1.f_out, config2.f_out))\n",
    "            if config2.f_out != n_actions:\n",
    "                fc_layers.append(nn.ReLU())\n",
    "            if config2.dropout:\n",
    "                fc_layers.append(nn.Dropout(config2.dropout))\n",
    "                \n",
    "        return nn.Sequential(*fc_layers)\n",
    "\n",
    "    def one_hot_state(\n",
    "            self,\n",
    "            state: ndarray[int64]\n",
    "        ) -> tuple:\n",
    "        one_hot_chars = torch.zeros((DQN.n_char_channels, Wordle.max_attempts, self.target_len))\n",
    "        one_hot_hints = torch.zeros((DQN.n_hint_channels, Wordle.max_attempts, self.target_len))\n",
    "        reached_attempts_made = False\n",
    "        for attempt_i in range(Wordle.max_attempts):\n",
    "            for pos_i in range(self.target_len):\n",
    "                space_tuple = (attempt_i, pos_i)\n",
    "                char, hint = state[:, *space_tuple]\n",
    "                if char == Wordle.initial_empty:\n",
    "                    reached_attempts_made\n",
    "                    break\n",
    "                one_hot_chars[char, *space_tuple] = 1.\n",
    "                one_hot_hints[hint, *space_tuple] = 1.\n",
    "            if reached_attempts_made:\n",
    "                break\n",
    "        return (one_hot_chars, one_hot_hints)\n",
    "\n",
    "    def train_model(self, trainer: DQNTrainer) -> training_history:\n",
    "        target_net = DQN(\n",
    "            self.target_len,\n",
    "            self.n_actions,\n",
    "            self.conv_pool_configs,\n",
    "            self.fc_configs\n",
    "        )\n",
    "        target_net.load_state_dict(self.state_dict())\n",
    "        trainer.set_target_net(target_net)\n",
    "        return trainer.train(self)\n",
    "\n",
    "    def get_reward(self, state, status, guess_n) -> float:\n",
    "        turn_value = 10.\n",
    "        reward = -turn_value\n",
    "        reward += sum(\n",
    "            1./self.target_len for hint in state[Wordle.hint_channel, guess_n - 1]\n",
    "            if hint == Wordle.correct\n",
    "        )\n",
    "        reward += Wordle.max_attempts * turn_value * int(status == Wordle.won)\n",
    "        return reward / self.target_len\n",
    "\n",
    "\n",
    "class DQNTrainer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        memory_size: int = 10_000,\n",
    "        n_episodes: int = 10_000,\n",
    "        checkpointer: Checkpointer = None,\n",
    "        stopper: EarlyStopper = None,\n",
    "        device: torch.device = None,\n",
    "        batch_size: int = 128,\n",
    "        discount: float = 0.99,\n",
    "        eps_start: float = 0.9,\n",
    "        eps_end: float = 0.05,\n",
    "        eps_decay: int = 1000,\n",
    "        update_rate: float = 1e-4,\n",
    "    ) -> None:\n",
    "        self.optimizer = optimizer\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        self.n_episodes = n_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.update_rate = update_rate\n",
    "        self.checkpointer = checkpointer\n",
    "        self.stopper = stopper\n",
    "        self.device = device\n",
    "\n",
    "        self.step_n = 0\n",
    "        self.batch_n = 0\n",
    "\n",
    "        self.target_net = None # set in DQN.train w/ DQNTrainer.set_target_net\n",
    "\n",
    "    def set_target_net(self, target_net: DQN) -> None:\n",
    "        self.target_net = target_net\n",
    "        self.target_net.to(self.device)\n",
    "\n",
    "    def select_action(self, policy_net: DQN, state: torch.Tensor) -> torch.Tensor:\n",
    "        self.step_n += 1\n",
    "        eps_threshold = (\n",
    "            (self.eps_start - self.eps_end)\n",
    "            * math.exp(-1. * self.steps_n / self.eps_decay)\n",
    "            + self.eps_end \n",
    "        )\n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor(\n",
    "                [[random.randint(0, policy_net.n_actions - 1)]],\n",
    "                device=self.device,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "    def train(self, policy_net: DQN, n_episodes: int = 0) -> training_history:\n",
    "        if n_episodes == 0:\n",
    "            n_episodes == self.n_episodes\n",
    "\n",
    "        episode_rewards = []\n",
    "        losses = []\n",
    "        for episode_i in range(n_episodes):\n",
    "            #print(f'{episode_i = }')\n",
    "            if self.stopper.stopped:\n",
    "                print(f'Early stopping after {episode_i} episodes')\n",
    "                break\n",
    "            wordle = Wordle(all_words)\n",
    "            #print(f'{wordle.target = }')\n",
    "            #print(f'{wordle.target_len = }')\n",
    "            state = wordle.state\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            #next_state = state\n",
    "            status = Wordle.ongoing\n",
    "            episode_reward = 0\n",
    "            while status == Wordle.ongoing:\n",
    "                #print(f'state size in loop: {state.size()}')\n",
    "                #print(f'state in loop: {state}')\n",
    "                action = select_action(policy_net, state)\n",
    "                invalid_guess = False\n",
    "                try:\n",
    "                    next_state = wordle.guess(all_words[action])\n",
    "                    status = wordle.check_state()\n",
    "                except ValueError:\n",
    "                    # \"Should\" maybe wordle.attempts_made += 1, but not used in reward anyway\n",
    "                    next_state = None\n",
    "                    status = Wordle.lost\n",
    "                    invalid_guess = True\n",
    "                reward = get_reward(\n",
    "                    next_state,\n",
    "                    status,\n",
    "                    wordle.attempts_made,\n",
    "                    target_len\n",
    "                )\n",
    "                episode_reward += reward\n",
    "                if not invalid_guess:\n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "        \n",
    "                # Perform optimization on the policy network, checkpoint, early stopping\n",
    "                loss = self.optimize_model()\n",
    "                if loss:\n",
    "                    losses.append(loss)\n",
    "                    batch_n += 1\n",
    "                    self.checkpointer.checkpoint(loss, policy_net, optimizer)\n",
    "                if self.stopper.stop(loss, episode_i):\n",
    "                    break\n",
    "        \n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = (\n",
    "                        policy_net_state_dict[key] * self.update_rate\n",
    "                        + target_net_state_dict[key] * (1 - self.update_rate)\n",
    "                    )\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "                if status != Wordle.ongoing:\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    #if episode_i % 50 == 0:\n",
    "                    plot_training(losses, batch_n, episode_rewards, episode_i)\n",
    "\n",
    "        return {\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'losses': losses\n",
    "        }\n",
    "\n",
    "    def optimize_model(self) -> Union[None, float]:\n",
    "        \"\"\" Return Huber loss \"\"\"\n",
    "        if len(memory) < self.batch_size:\n",
    "            return None\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "    \n",
    "        # Compute a mask of non-terminal states and concatenate the batch elements\n",
    "        non_terminal_mask = torch.logical_not(torch.tensor(batch.is_terminal, device=device, dtype=torch.bool))\n",
    "        non_terminal_next_states = [s for s, nt in zip(batch.next_state, non_terminal_mask) if nt]\n",
    "        #if not non_terminal_nest_states:\n",
    "        #    return None\n",
    "        non_terminal_next_states = torch.cat(non_terminal_nest_states)\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "        # Compute Q(s_t, a)\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_terminal_mask] = target_net(non_terminal_next_states).max(1).values\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = torch.where(\n",
    "            non_terminal_mask,\n",
    "            reward_batch + next_state_values * self.discount,\n",
    "            reward_batch\n",
    "        )\n",
    "    \n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "        return float(loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(losses: list[float], batch_n: int, episode_rewards: list[float], episode_n: int):\n",
    "        plt.figure(0, figsize=(20, 8))\n",
    "        plt.clf()\n",
    "        plt.title(f'Episode {episode_n}')\n",
    "    \n",
    "        if batch_n:\n",
    "            plt.delaxes()\n",
    "            ax1 = plt.subplot(121)\n",
    "            ax2 = plt.subplot(122)\n",
    "            \n",
    "            ax1.set_xlabel('Batch Number')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.scatter(tuple(range(batch_n)), losses)\n",
    "    \n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Total Reward')\n",
    "            ax2.scatter(tuple(range(episode_n + 1)), episode_rewards)\n",
    "    \n",
    "            plt.tight_layout()\n",
    "    \n",
    "        else:\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Total Reward')\n",
    "            plt.scatter(tuple(range(episode_n + 1)), episode_rewards)\n",
    "    \n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c1e6798-99af-4bce-acae-0b5b5ad91e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LEN = 5\n",
    "ALL_WORDS = load_vocab(f'../corncob_caps_5.txt')\n",
    "N_ACTIONS = len(ALL_WORDS)\n",
    "\n",
    "conv_pool_configs = [\n",
    "    # ConvPoolConfig(conv_out, conv_kernel_size, pool_kernel_size, pool_stride, dropout)\n",
    "    ConvPoolConfig(DQN.channels_in, 1,           0, 0,                          0.05),\n",
    "    ConvPoolConfig(DQN.channels_in * 4, 3,       (1, 3), (0, 1),                0.1),\n",
    "    ConvPoolConfig(DQN.channels_in * 8, 3,       (1, 2), (0, 1),                0.1)\n",
    "]\n",
    "fcin = DQN.get_fcin(\n",
    "    DQN.get_input_size(TARGET_LEN),\n",
    "    DQN.make_conv_pool_layers(conv_pool_configs)\n",
    ")\n",
    "fc_configs = [\n",
    "    FCConfig(2 ** 12, 0.5),\n",
    "    FCConfig(N_ACTIONS, 0.)\n",
    "]\n",
    "\n",
    "wordle_net = DQN(TARGET_LEN, N_ACTIONS, conv_pool_configs, fc_configs).to(device)\n",
    "\n",
    "lr = 1e-5\n",
    "optimizer = optim.AdamW(wordle_net.parameters(), lr=lr, amsgrad=True)\n",
    "checkpointer = Checkpointer('../checkpoints', 'model_5c.pt', 'optim_5c.pt')\n",
    "checkpointer.checkpoint(None, wordle_net, optimizer)\n",
    "stopper = EarlyStopper(\n",
    "    min_delta=0.0,\n",
    "    patience=50,\n",
    "    start_episode=200\n",
    ")\n",
    "\n",
    "trainer = DQNTrainer(\n",
    "    optimizer,\n",
    "    checkpointer=checkpointer,\n",
    "    stopper=stopper,\n",
    "    device=device\n",
    ")\n",
    "wordle_net.train_model(trainer)\n",
    "    \n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460e164-1ae6-41b6-92c2-6e7374aeed78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c91c76-14f5-42c1-af2d-2bcd1ab180f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Wordle(all_words)\n",
    "print(f'{game.target = }')\n",
    "print(f'{game.target_len = }')\n",
    "state = game.state\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    action = policy_net(state).max(1).indices.view(1, 1)\n",
    "    guess = all_words[action]\n",
    "print(f'guessing: {all_words[action]}')\n",
    "state = game.guess(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a885358-052a-4576-9424-1bfa934dec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "npn = DQN(n_actions)\n",
    "npn.load_state_dict(torch.load('../checkpoints/model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
