{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184a04ae-a62d-45ec-b226-cbbf5eb975f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e1592f-1e2b-4bf0-aa2d-fa4ff6e2db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'is_terminal'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e53196-0e47-49ba-89b3-2d50f1e19079",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DQN.__init__() missing 2 required positional arguments: 'conv_pool_configs' and 'fc_configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m    136\u001b[0m target_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m--> 138\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    139\u001b[0m target_net \u001b[38;5;241m=\u001b[39m DQN(target_len, n_actions)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    140\u001b[0m target_net\u001b[38;5;241m.\u001b[39mload_state_dict(policy_net\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "\u001b[0;31mTypeError\u001b[0m: DQN.__init__() missing 2 required positional arguments: 'conv_pool_configs' and 'fc_configs'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = '/'.join(os.getcwd().split('/')[:-1])\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from wordle import Wordle\n",
    "from wordle import load_vocab\n",
    "from wordle import ALPH_LEN\n",
    "\n",
    "from itertools import pairwise\n",
    "from numpy import prod\n",
    "\n",
    "from typing import Iterable\n",
    "from numpy import ndarray\n",
    "from numpy import int64\n",
    "\n",
    "\n",
    "all_words = load_vocab(f'../corncob_caps_5.txt')\n",
    "n_actions = len(all_words)\n",
    "\n",
    "\n",
    "ConvPoolConfig = namedtuple(\n",
    "    'ConvPoolConfig',\n",
    "    (\n",
    "        'conv_in',\n",
    "        'conv_out',\n",
    "        'conv_kernel_size',\n",
    "        'pool_kernel_size',\n",
    "        'pool_padding'\n",
    "    )\n",
    ")\n",
    "\n",
    "FCConfig = namedtuple('FCConfig', ('f_out', 'dropout'))\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    n_char_channels = 1 + ALPH_LEN\n",
    "    n_hint_channels = Wordle.n_hint_states\n",
    "    channels_in = n_char_channels + n_hint_channels\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_len: int,\n",
    "        n_out: int,\n",
    "        conv_pool_configs: Iterable[ConvPoolConfig],\n",
    "        fc_configs: list[FCConfig]\n",
    "    ) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        self.target_len = target_len\n",
    "        self.n_out = n_out\n",
    "        if self.n_out != fc_configs[-1].f_out:\n",
    "            raise ValueError(f'fc_configs[-1].f_out ({fc_configs[-1].f_out}) should equal n_out ({n_out})')\n",
    "\n",
    "        self.conv_pool_layers = DQN.make_conv_pool_layers(self.target_len, conv_pool_configs)\n",
    "        self.fcin = DQN.get_fcin(\n",
    "            input_size=(DQN.channels_in, Wordle.max_attempts, self.target_len),\n",
    "            conv_pool_layers=self.conv_pool_layers\n",
    "        )\n",
    "        self.fc_layers = DQN.make_fc_layers(fc_configs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.one_hot_state(x)\n",
    "        x = torch.cat(x)\n",
    "        x = self.conv_pool_layers(x)\n",
    "        x = x.view(-1, self.fcin)  # -1 for batch size inference\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def make_conv_pool_layers(conv_pool_configs: Iterable[ConvPoolConfig]) -> nn.Sequential:\n",
    "        conv_pool_layers = []\n",
    "        for config in conv_pool_configs:\n",
    "            conv_pool_layers.extend(\n",
    "                nn.Conv2d(config.conv_in, config.conv_out, kernel_size=config.conv_kernel_size, padding='same'),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            if config.pool_kernel_size:\n",
    "                conv_pool_layers.append(\n",
    "                    nn.MaxPool2d(kernel_size=config.pool_kernel_size, stride=1, padding=config.pool_padding)\n",
    "                )\n",
    "            if config.dropout:\n",
    "                conv_pool_layers.append(nn.Dropout2d(config.dropout))\n",
    "        return nn.Sequential(*conv_pool_layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fcin(input_size: tuple[int], conv_pool_layers: nn.Sequential) -> int:\n",
    "        test_tensor = torch.zeros(input_size)\n",
    "        with torch.no_grad():\n",
    "            test_out_size = conv_pool_layers(test_tensor).size()\n",
    "        return prod(test_out_size)\n",
    "\n",
    "    def make_fc_layers(self, fc_configs: list[FCConfig]) -> nn.Sequential:\n",
    "        fc_layers = [nn.Linear(fcin, fc_configs[0].f_out)]\n",
    "        if fc_configs[0].dropout:\n",
    "            fc_layers.append(nn.Dropout(fc_configs[0].dropout))\n",
    "        for config1, config2 in pairwise(fc_configs):\n",
    "            fc_layers.append(nn.Linear(config1.f_out, config2.f_out))\n",
    "            if config1.dropout:\n",
    "                fc_layers.append(nn.Dropout(config1.dropout))\n",
    "        return nn.Sequential(*fc_layers)\n",
    "\n",
    "    def one_hot_state(\n",
    "            self,\n",
    "            state: ndarray[int64]\n",
    "        ) -> tuple:\n",
    "        one_hot_chars = torch.zeros((DQN.n_char_channels, Wordle.max_attempts, target_len))\n",
    "        one_hot_hints = torch.zeros((DQN.n_hint_channels, Wordle.max_attempts, target_len))\n",
    "        reached_attempts_made = False\n",
    "        for attempt_i in range(Wordle.max_attempts):\n",
    "            for pos_i in range(self.target_len):\n",
    "                space_tuple = (attempt_i, pos_i)\n",
    "                char, hint = state[:, *space_tuple]\n",
    "                if char == Wordle.initial_empty:\n",
    "                    reached_attempts_made\n",
    "                    break\n",
    "                one_hot_chars[char, *space_tuple] = 1.\n",
    "                one_hot_hints[hint, *space_tuple] = 1.\n",
    "            if reached_attempts_made:\n",
    "                break\n",
    "        return (one_hot_chars, one_hot_hints)\n",
    "\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the terminal value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 1e-4\n",
    "LR = 1e-5\n",
    "target_len = 5\n",
    "\n",
    "policy_net = DQN(target_len, n_actions).to(device)\n",
    "target_net = DQN(target_len, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(policy_net, game):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        char_hint_states = game.get_one_hot_state()\n",
    "        with torch.no_grad():\n",
    "            return policy_net(*char_hint_states).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor(\n",
    "            [[random.randint(0, n_actions - 1)]],\n",
    "            device=device,\n",
    "            dtype=torch.long\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70aa05-9d5e-4a92-a185-e4ff92c3dabf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def optimize_model() -> Union[None, float]:\n",
    "    \"\"\" Return Huber loss \"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return None\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-terminal states and concatenate the batch elements\n",
    "    # (a terminal state would've been the one after which simulation ended)\n",
    "    non_terminal_mask = torch.logical_not(torch.tensor(batch.is_terminal, device=device, dtype=torch.bool))\n",
    "    non_terminal_next_states = [s for s, nt in zip(batch.next_state, non_terminal_mask) if nt]\n",
    "    #if not non_terminal_nest_states:\n",
    "    #    return None\n",
    "    non_terminal_next_states = torch.cat(non_terminal_nest_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_terminal_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was terminal\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_terminal_mask] = target_net(non_terminal_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = torch.where(\n",
    "        non_terminal_mask,\n",
    "        reward_batch + next_state_values * GAMMA,\n",
    "        reward_batch\n",
    "    )\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e6798-99af-4bce-acae-0b5b5ad91e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 300\n",
    "\n",
    "def get_reward(state, status, guess_n, target_len) -> float:\n",
    "    reward = -10\n",
    "    reward += sum(\n",
    "        1./target_len for hint in state[Wordle.hint_channel, guess_n - 1]\n",
    "        if hint == Wordle.correct\n",
    "    )\n",
    "    reward += 60 * int(status == Wordle.won)\n",
    "    return reward / target_len\n",
    "\n",
    "def plot_history(losses: list[float], batch_n: int, rewards: list[float], episode_n: int):\n",
    "    plt.figure(0, figsize=(20, 8))\n",
    "    plt.clf()\n",
    "    plt.title(f'Episode {episode_n}')\n",
    "\n",
    "    if batch_n:\n",
    "        plt.delaxes()\n",
    "        ax1 = plt.subplot(121)\n",
    "        ax2 = plt.subplot(122)\n",
    "        \n",
    "        ax1.set_xlabel('Batch Number')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.scatter(tuple(range(batch_n)), losses)\n",
    "\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Total Reward')\n",
    "        ax2.scatter(tuple(range(episode_n + 1)), rewards)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    else:\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.scatter(tuple(range(episode_n + 1)), rewards)\n",
    "\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "class Checkpointer():\n",
    "    def __init__(self, dir, model_file, optim_file):\n",
    "        self.dir = dir\n",
    "        self.model_file = os.path.join(self.dir, model_file)\n",
    "        self.optim_file = os.path.join(self.dir, optim_file)\n",
    "        self.best_loss = None\n",
    "\n",
    "    def checkpoint(self, loss, model, optim) -> None:\n",
    "        if self.best_loss is None or loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            model.eval()\n",
    "            torch.save(model.state_dict(), self.model_file)\n",
    "            torch.save(optim.state_dict(), self.optim_file)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, min_delta=0, patience=0, start_episode=0):\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.start_episode = start_episode\n",
    "        self.episode_last_improved = 0\n",
    "        self.best_loss = None\n",
    "        self.stopped = False\n",
    "\n",
    "    def stop(self, loss, episode_n) -> bool:\n",
    "        if self.best_loss is None or loss <= self.loss - self.min_delta:\n",
    "            self.loss = loss\n",
    "            self.episode_last_improved = episode_n\n",
    "            return False\n",
    "        elif (\n",
    "            episode_n > start_episode + self.patience\n",
    "            and episode_n > sef.episode_last_improved + self.patience\n",
    "        ):\n",
    "            self.stopped = True\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "rewards = []\n",
    "losses = []\n",
    "batch_n = 0\n",
    "checkpointer = Checkpointer('../checkpoints', 'model_5c.pt', 'optim_5c.pt')\n",
    "checkpointer.checkpoint(None, policy_net, optimizer)\n",
    "stopper = EarlyStopper(\n",
    "    min_delta=0.0,\n",
    "    patience=50,\n",
    "    start_episode=200\n",
    ")\n",
    "for i_episode in range(num_episodes):\n",
    "    #print(f'{i_episode = }')\n",
    "    if stopper.stopped:\n",
    "        print(f'Early stopping after {i_episode} episodes')\n",
    "        break\n",
    "    wordle = Wordle(all_words)\n",
    "    #print(f'{wordle.target = }')\n",
    "    #print(f'{wordle.target_len = }')\n",
    "    state = wordle.state\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    #next_state = state\n",
    "    status = Wordle.ongoing\n",
    "    reward_total = 0\n",
    "    while status == Wordle.ongoing:\n",
    "        #print(f'state size in loop: {state.size()}')\n",
    "        #print(f'state in loop: {state}')\n",
    "        action = select_action(policy_net, state)\n",
    "        invalid_guess = False\n",
    "        try:\n",
    "            next_state = wordle.guess(all_words[action])\n",
    "            status = wordle.check_state()\n",
    "        except ValueError:\n",
    "            # \"Should\" maybe wordle.attempts_made += 1, but not used in reward anyway\n",
    "            next_state = None\n",
    "            status = Wordle.lost\n",
    "            invalid_guess = True\n",
    "        reward = get_reward(\n",
    "            next_state,\n",
    "            status,\n",
    "            wordle.attempts_made,\n",
    "            target_len\n",
    "        )\n",
    "        reward_total += reward\n",
    "        if not invalid_guess:\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        # Perform optimization on the policy network, checkpoint, early stopping\n",
    "        loss = optimize_model()\n",
    "        if loss:\n",
    "            loss = float(loss)\n",
    "            losses.append(loss)\n",
    "            batch_n += 1\n",
    "            checkpointer.checkpoint(loss, policy_net, optimizer)\n",
    "        if stopper.stop(loss, i_episode):\n",
    "            break\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if status != Wordle.ongoing:\n",
    "            rewards.append(reward_total)\n",
    "            #if i_episode % 50 == 0:\n",
    "            plot_history(losses, batch_n, rewards, i_episode)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print('Complete')\n",
    "plot_history(losses, batch_n, rewards, i_episode);\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460e164-1ae6-41b6-92c2-6e7374aeed78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c91c76-14f5-42c1-af2d-2bcd1ab180f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Wordle(all_words)\n",
    "print(f'{game.target = }')\n",
    "print(f'{game.target_len = }')\n",
    "state = game.state\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    action = policy_net(state).max(1).indices.view(1, 1)\n",
    "    guess = all_words[action]\n",
    "print(f'guessing: {all_words[action]}')\n",
    "state = game.guess(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a885358-052a-4576-9424-1bfa934dec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "npn = DQN(n_actions)\n",
    "npn.load_state_dict(torch.load('../checkpoints/model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
